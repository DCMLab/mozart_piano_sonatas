{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../Code'))\n",
    "from piece.piece import *\n",
    "from helpers import *\n",
    "import re, mmap, csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell serves to align all markdown tables to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html \n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Your data needs to be in one separated values file per piece in the corpus and should at least have the columns\n",
    "\n",
    "|measure|beat|label|\n",
    "|-------|----|-----|\n",
    "\n",
    "If you want to link the data to another dataset or to the music scores, the file names (but not the file extensions) need to be identical.\n",
    "\n",
    "You further need a dataframe that holds the time signatures for all measures of all pieces in the format\n",
    "\n",
    "|**measure**|piece1|piece2|...|\n",
    "|-----------|------|------|---|\n",
    "| 0         | 4/4  | 6/8  |...|\n",
    "| 1         | 4/4  | 6/8  |...|\n",
    "|...        | ...  | ...  |...|\n",
    "\n",
    "Further down you can create such a time signature map from MuseScore2 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "musescore = 'mscore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsms(score_dir):\n",
    "    \"\"\" Extract the Time Signature Maps from all MSCX files in score_dir\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    pd.DataFrame:\n",
    "        Where the index are measure numbers and every column holds the time signatures for one piece\n",
    "    \"\"\"\n",
    "    all_tsms = {}\n",
    "    for file in os.listdir(score_dir):\n",
    "        if file.endswith('.mscx'):\n",
    "            filename = os.path.splitext(file)[0] \n",
    "            path = os.path.join(score_dir,file)\n",
    "            p = Piece(path,timesig_map_only=True,ms=musescore)\n",
    "            #measure_count = max(list(timesigs.keys()))\n",
    "            all_tsms[filename] = p.get_timesig_map()\n",
    "    return pd.DataFrame.from_dict(all_tsms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get time signature maps\n",
    "\n",
    "### You can either extract them from a set of MuseScore2 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measure number 36 should be corrected to 38\n",
      "m. 53 is incomplete and m. 54 does not complete it. Correct manually using MuseScores 'Bar Properties'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c03fceae469e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmusescorefiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./scores'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtsms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tsms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmusescorefiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtsms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_signature_maps.tsv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtsms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7534c8089119>\u001b[0m in \u001b[0;36mget_tsms\u001b[0;34m(score_dir)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimesig_map_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmusescore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;31m#measure_count = max(list(timesigs.keys()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mall_tsms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_timesig_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/piece/piece.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mscx, repair, ms, timesig_map_only)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;31m### Object representing the XML structure to be altered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirststaff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Part'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_next_sibling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Staff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherstaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirststaff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_next_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Staff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParserError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._handleSaxTargetStart\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._callTargetSaxStart\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._PythonSaxParserTarget._handleSaxStart\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, name, attrs, nsmap)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getNsTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mnsprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefix_for_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prefix_for_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mhandle_starttag\u001b[0;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \"\"\"\n\u001b[1;32m    623\u001b[0m         \u001b[0;31m# print \"Start tag %s: %s\" % (name, attrs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         if (self.parse_only and len(self.tagStack) <= 1\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mendData\u001b[0;34m(self, containerClass)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainerClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_was_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobject_was_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost_recent_element\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mobject_was_parsed\u001b[0;34m(self, o, parent, most_recent_element)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# Check if we are inserting into an already parsed node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linkage_fixer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_linkage_fixer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_linkage_fixer\u001b[0;34m(self, el)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# This index is a tag, dig deeper for a \"last descendant\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0mdescendant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_descendant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "musescorefiles = './scores'\n",
    "\n",
    "tsms = get_tsms(musescorefiles)\n",
    "tsms.to_csv('time_signature_maps.tsv',sep='\\t')\n",
    "tsms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or load existing time signature maps\n",
    "\n",
    "The desired structure is a TSV file with measure numbers as indices and piece names as columns, so as to know a time signature for every measure of every piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesignaturemaps = 'time_signature_maps.tsv'\n",
    "\n",
    "tsms = pd.read_csv(timesignaturemaps,sep='\\t',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract harmony labels from a folder of MSCX (uncompressed MuseScore 2) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./chords/K311-1.mscx\n",
      "All good.\n",
      "./chords/K311-1.txt written.\n"
     ]
    }
   ],
   "source": [
    "scoredir = './chords'\n",
    "\n",
    "\n",
    "def extract(dir,goal='',repair=False,files_re=r'.*\\.mscx$',recursive=False,remove=None,metadata=False):\n",
    "    \"\"\"Extract harmony labels from all uncompressed MuseScore2 files (*mscx) in *dir*, correct them, and save them to a TXT file.\n",
    "\n",
    "    Optionally, remove the labels from the file and save as an empty one.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    dir: str\n",
    "        path to the directory with MSCX files\n",
    "    repair: bool, optional\n",
    "        if you additionally want to save autocorrections to a new MSCX file\n",
    "    files_re: rString, optional\n",
    "        regex to select only certain pieces\n",
    "    recursive: bool, optional\n",
    "        include sub-directories\n",
    "    remove: str, optional\n",
    "        if you want to save an empty file after extraction, pass the suffix for the new file, e.g.\n",
    "        ``remove='_clean'``\n",
    "    \"\"\"\n",
    "    for subdir, dirs, files in os.walk(dir):\n",
    "        if not recursive:\n",
    "            dirs[:] = []\n",
    "        for file in sorted(files):\n",
    "            m = re.search(files_re,file)\n",
    "            if m and re.search(r'.*\\.mscx$',file):\n",
    "                path = os.path.join(subdir,file)\n",
    "                print(\"Processing \" + path)\n",
    "\n",
    "                with open(path, 'rb', 0) as f, mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as s:\n",
    "                    if s.find(b'<Harmony>') == -1:\n",
    "                        print(f\"{file} contains no labels. Skipped.\")\n",
    "                        continue\n",
    "\n",
    "                p = Piece(path,repair,ms=musescore)\n",
    "                key = p.key\n",
    "                erroneous = p.get_harmonies(True)\n",
    "                if len(erroneous) > 1 or key == '':\n",
    "                    msg = f\"{file}'s syntax contains errors. Skipped.\"\n",
    "                    if not repair:\n",
    "                        msg += \" Try option -r for autorepair.\"\n",
    "                    print(msg)\n",
    "                    continue\n",
    "                all = p.get_harmonies()\n",
    "                if goal != '':\n",
    "                    subdir = goal\n",
    "                n = re.search(r'(.*)(\\..*)$',file)\n",
    "                piece = n.group(1)\n",
    "                if repair:\n",
    "                    piece += \"_repaired\"\n",
    "                txt = os.path.join(subdir, piece + '.txt')\n",
    "                with open(txt, 'w') as tsvfile:\n",
    "                    if metadata:\n",
    "                        tsvfile.write('@skip: 4\\n')\n",
    "                        tsvfile.write(f'@piece: {piece}\\n')\n",
    "                        tsvfile.write(f'@key: {key}\\n')\n",
    "                        tsvfile.write(f'@meter: {p.timesig}\\n')\n",
    "                    tsvfile.write('measure\\tbeat\\tlabel\\n')\n",
    "                    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "                    for l in all:\n",
    "                        writer.writerow(l)\n",
    "                print(txt + ' written.')\n",
    "                if remove:\n",
    "                    p.remove_harmonies(os.path.join(subdir,piece+remove+n.group(2)))\n",
    "                    \n",
    "extract(scoredir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a continuous position to every entry\n",
    "\n",
    "### Specify the folder where the data lies and the folder for the expanded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on K311-1.txt\n"
     ]
    }
   ],
   "source": [
    "#data = './tests'\n",
    "data = './chords'\n",
    "#data = './cadences'\n",
    "\n",
    "#newset = './tests/new'\n",
    "newset = './chords_expanded'\n",
    "#newset = './cadences_expanded'\n",
    "\n",
    "def add_positions(dir, new_dir, time_signatures, files_re=\".*\", recursive=False, extensions=['tsv', 'txt', 'csv'], sep=['\\t', '\\t', ',']):\n",
    "    \"\"\" For every data entry, a decimal position is calculated from `measure` and `beat` and added as a column.\n",
    "    \n",
    "    For this, we need a dataframe with time signature maps (tsms, see above). The time signatures are added in another column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    dir: str\n",
    "        Directory with the files to process.\n",
    "    new_dir: str\n",
    "        Directory where to save the processed files.\n",
    "    time_signatures: pd.DataFrame\n",
    "        Index are measure numbers, columns are pieces, entries are strings such as '4/4' or '3/8'.\n",
    "    new_sizes: list or tuple\n",
    "        Corresponding new beat sizes.\n",
    "    files_re: str\n",
    "        In case you want to process only files where this regular expression finds a match.\n",
    "    recursive : :obj:`bool`, optional\n",
    "        Scan subdirectories as well? Defaults to `True`.\n",
    "    extensions : :obj:`list` of :obj:`str`, optional\n",
    "        File extensions to consider. Defaults to `['tsv','csv']`.\n",
    "        If the list is shorter than `extensions`, the last element is used.\n",
    "    sep : :obj:`list` of :obj:`str`, optional\n",
    "        The separator symbols corresponding to the extensions. Defaults to `['\\t',',']`.\n",
    "    \"\"\"\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(dir):\n",
    "        dirs.sort()\n",
    "        if not recursive:\n",
    "            dirs[:] = []\n",
    "\n",
    "        exts = '|'.join(extensions)\n",
    "        for file in files:\n",
    "      \n",
    "            m = re.match(f'(.+)\\.({exts})$',file)\n",
    "            if m and re.search(files_re,file):\n",
    "          \n",
    "                name = m[1]\n",
    "                ext  = m[2]\n",
    "                ind = extensions.index(ext)\n",
    "                s = sep[-1] if ind >= len(sep) else sep[ind]\n",
    "                path = os.path.join(subdir,file)\n",
    "                df = pd.read_csv(path, sep=s, error_bad_lines=False, warn_bad_lines=True)\n",
    "                print(\"Working on \" + file)\n",
    "                def add_decimals(r):\n",
    "                    timesig = time_signatures.loc[r.measure][name]\n",
    "                    r.loc['position'] =  measure_decimal(r.measure,r.beat,timesig,decimals=2)\n",
    "                    r.loc['timesig'] = timesig\n",
    "                    return r\n",
    "                df = df.apply(add_decimals,axis=1)\n",
    "                \n",
    "                df['duration'] = pd.Series([])\n",
    "                df['duration'].iloc[:-1] =  (df.position[1:].values - df.position[:-1].values).round(2)\n",
    "\n",
    "                df.to_csv(os.path.join(new_dir,file),sep='\\t',index=False)\n",
    "\n",
    "add_positions(data,newset,tsms,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt representation of beats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_beatsize(beat,orig_beatsize=4,new_beatsize=4):\n",
    "    \"\"\"Convert one beat size to another.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    b: str\n",
    "        Beat in the shape beat.subbeatFraction\n",
    "    orig_beatsize, new_beatsize: int\n",
    "        Old and new sizes of the beat, e.g. 8, 4\n",
    "\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "\n",
    "    >>> change_beatsize('1.1/2',2)\n",
    "    '2'\n",
    "    >>> change_beatsize('4',8)\n",
    "    '2.1/2'\n",
    "\n",
    "    \"\"\"\n",
    "     \n",
    "    b = beat2float(beat)\n",
    "    scale = new_beatsize/orig_beatsize\n",
    "    if scale > 1:\n",
    "        new_b = b * scale - (scale - 1)\n",
    "    elif scale < 1:\n",
    "        new_b = (b + (1 / scale - 1)) * scale\n",
    "    else:\n",
    "        return fractionize(beat)\n",
    "    return fractionize(new_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './cadences_expanded'\n",
    "newset = './cadences_expanded'\n",
    "\n",
    "\n",
    "def adapt_beats(dir, new_dir, orig_sizes = [2,8], new_sizes = [4,4], recursive=False, extensions=['tsv', 'txt', 'csv'], sep=['\\t', '\\t', ',']):\n",
    "    \"\"\" Apply the function `change_beatsize` to all rows of all files in `dir`\n",
    "    if the time signature's denominator is included in `orig_sizes`.\n",
    "    \n",
    "    For this to work, the dataset has to have been expanded in the section\n",
    "    \"Add a continuous position to every entry\" above, so it holds a column\n",
    "    named \"timesig\".\n",
    "    \n",
    "    As a result, all beats will be converted to strings of the shape beat.subbeatFraction\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    dir: str\n",
    "        Directory with the files to process.\n",
    "    new_dir: str\n",
    "        Directory where to save the processed files.\n",
    "    orig_sizes: list or tuple\n",
    "        All beat sizes / denominators you want to alter.\n",
    "    new_sizes: list or tuple\n",
    "        Corresponding new beat sizes.\n",
    "    recursive : :obj:`bool`, optional\n",
    "        Scan subdirectories as well? Defaults to `True`.\n",
    "    extensions : :obj:`list` of :obj:`str`, optional\n",
    "        File extensions to consider. Defaults to `['tsv','csv']`.\n",
    "        If the list is shorter than `extensions`, the last element is used.\n",
    "    sep : :obj:`list` of :obj:`str`, optional\n",
    "        The separator symbols corresponding to the extensions. Defaults to `['\\t',',']`.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(dir):\n",
    "        dirs.sort()\n",
    "        if not recursive:\n",
    "            dirs[:] = []\n",
    "\n",
    "        exts = '|'.join(extensions)\n",
    "        for file in files:\n",
    "            m = re.match(f'(.+)\\.({exts})$',file)\n",
    "            if m:\n",
    "                name = m[1]\n",
    "                ext  = m[2]\n",
    "                ind = extensions.index(ext)\n",
    "                s = sep[-1] if ind >= len(sep) else sep[ind]\n",
    "                path = os.path.join(subdir,file)\n",
    "                df = pd.read_csv(path, sep=s, error_bad_lines=False, warn_bad_lines=True)\n",
    "                \n",
    "                def quarter_beats(r):\n",
    "                    denom = int(r.timesig.split('/')[1])\n",
    "                    if denom in orig_sizes:\n",
    "                        r.beat = change_beatsize(r.beat,denom,new_sizes[orig_sizes.index(denom)])\n",
    "                    else:\n",
    "                        r.beat = fractionize(r.beat)\n",
    "                    return r\n",
    "                df = df.apply(quarter_beats, axis=1)\n",
    "                \n",
    "                df.to_csv(os.path.join(new_dir,file),sep='\\t',index=False)\n",
    "\n",
    "#adapt_beats(data,newset,[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './cadences_expanded'\n",
    "newset = './cadences_expanded'\n",
    "\n",
    "\n",
    "def adapt_types(dir, new_dir, types={'measure':int}, recursive=False, extensions=['tsv', 'txt', 'csv'], sep=['\\t', '\\t', ',']):\n",
    "    \"\"\" This function reads the files with the given `extensions`, reading values using the given `sep`arator(s) and applying\n",
    "    the `types` given as a dictionary. Then the files are stored using the new dtypes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    dir: str\n",
    "        Directory with the files to process.\n",
    "    new_dir: str\n",
    "        Directory where to save the processed files.\n",
    "    types: dict, optional\n",
    "        For every column you want to change, indicate the dtype.\n",
    "    recursive : :obj:`bool`, optional\n",
    "        Scan subdirectories as well? Defaults to `True`.\n",
    "    extensions : :obj:`list` of :obj:`str`, optional\n",
    "        File extensions to consider. Defaults to `['tsv','csv']`.\n",
    "        If the list is shorter than `extensions`, the last element is used.\n",
    "    sep : :obj:`list` of :obj:`str`, optional\n",
    "        The separator symbols corresponding to the extensions. Defaults to `['\\t',',']`.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(dir):\n",
    "        dirs.sort()\n",
    "        if not recursive:\n",
    "            dirs[:] = []\n",
    "\n",
    "        exts = '|'.join(extensions)\n",
    "        for file in files:\n",
    "            m = re.match(f'(.+)\\.({exts})$',file)\n",
    "            if m:\n",
    "                name = m[1]\n",
    "                ext  = m[2]\n",
    "                ind = extensions.index(ext)\n",
    "                s = sep[-1] if ind >= len(sep) else sep[ind]\n",
    "                path = os.path.join(subdir,file)\n",
    "                df = pd.read_csv(path, sep=s, error_bad_lines=False, warn_bad_lines=True, dtype=types)\n",
    "                df.to_csv(os.path.join(new_dir,file),sep='\\t',index=False)\n",
    "\n",
    "\n",
    "#adapt_types(data,newset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split values into two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = './cadences_expanded'\n",
    "data = './chords_expanded'\n",
    "#newset = './cadences_expanded'\n",
    "newset = './chords_expanded'\n",
    "\n",
    "\n",
    "def split_values(dir, new_dir, column, splitchar, names, drop=False, recursive=False, extensions=['tsv', 'txt', 'csv'], sep=['\\t', '\\t', ','], files_re='.*'):\n",
    "    \"\"\" Split the column `column` by the string `splitchar` and create additional columns named `names`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    dir: str\n",
    "        Directory with the files to process.\n",
    "    new_dir: str\n",
    "        Directory where to save the processed files.\n",
    "    column: str\n",
    "        Column to be split.\n",
    "    splitchar: str\n",
    "        String used as a separator\n",
    "    names: tuple or list\n",
    "        List with names for the new columns. Use existing column names to overwrite.\n",
    "    drop: bool, optional\n",
    "        If True, the split column will be dropped.\n",
    "    recursive : :obj:`bool`, optional\n",
    "        Scan subdirectories as well? Defaults to `True`.\n",
    "    extensions : :obj:`list` of :obj:`str`, optional\n",
    "        File extensions to consider. Defaults to `['tsv', 'txt', 'csv']`.\n",
    "    sep : :obj:`list` of :obj:`str`, optional\n",
    "        The separator symbols corresponding to the extensions. Defaults to `['\\t', '\\t', ',']`.\n",
    "        If the list is shorter than `extensions`, the last element is used.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(dir):\n",
    "        dirs.sort()\n",
    "        if not recursive:\n",
    "            dirs[:] = []\n",
    "\n",
    "        exts = '|'.join(extensions)\n",
    "        for file in files:\n",
    "            m = re.match(f'(.+)\\.({exts})$',file)\n",
    "            if m and re.search(files_re,file):\n",
    "                name = m[1]\n",
    "                ext  = m[2]\n",
    "                ind = extensions.index(ext)\n",
    "                s = sep[-1] if ind >= len(sep) else sep[ind]\n",
    "                path = os.path.join(subdir,file)\n",
    "                df = pd.read_csv(path, sep=s, error_bad_lines=False, warn_bad_lines=True)\n",
    "                \n",
    "                spl = df.loc[:,column].str.split(splitchar,len(names),expand=True)\n",
    "                for i,c in enumerate(spl.columns):\n",
    "                    df.loc[:, names[i]] = spl.loc[:,c]\n",
    "                if drop:\n",
    "                    df = df.drop(columns=column)\n",
    "                    \n",
    "                df.to_csv(os.path.join(new_dir,file),sep='\\t',index=False)\n",
    "\n",
    "split_values(data,newset,'label','-',['label','alt_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels expanded into separate columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laser/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py:887: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted all cadence sequences.\n",
      "Computed cadence stages.\n",
      "Added stage information to self.df\n",
      "Added neighbour indices to self.df\n",
      "Stored ./dataset.tsv\n",
      "Stored ./sequences.tsv\n",
      "Stored ./stages.tsv\n"
     ]
    }
   ],
   "source": [
    "from module import *\n",
    "cadencedir = './cadences_expanded/'\n",
    "\n",
    "chords = dataset(data,scoredir) \n",
    "cadences = dataset(cadencedir,scoredir)\n",
    "merged = merged_dataset(chords,cadences,compute_all=True)\n",
    "merged.dump(dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
